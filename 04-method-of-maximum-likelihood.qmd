# Beyond linear regression: The method of maximum likelihood

```{r}
pacman::p_load(tidyverse, GLMsData, janitor, patchwork)
```

## The idea of likelihood estimation

```{r}
data('quilpie')
quilpie <- quilpie |> as_tibble() |> clean_names() |> glimpse()
```

$$
P(y|\mu) = \mu^y (1-\mu)^{1-y},\\
\text{for } y=0 \text{ or } 1
$$

```{r}
mu <- seq(0, 1, 0.01492537)
ll <- rep(0, length(mu))

for (i in length(mu)) {
 ll[i] <- sum(
   dbinom(quilpie$y, size = 1, prob = mu[i], log = TRUE)
 ) 
}

likelihood <- 
  tibble(
    mu = mu,
    likelihood = ll
)
```

```{r}
likelihood |> 
  ggplot(aes(mu, likelihood)) +
  geom_point()
```

## Maximum likelihood for estimating one parameter

```{r}
# Maximum likelihood estimator
muhat <- mean(quilpie$y)

# Fisher's information
n <- length(quilpie$y)
info <- n / (muhat * (1 - muhat))

# Standard error of MLE
std <- 1 / sqrt(info)
  
c(
  'Mu:' = muhat,
  'Fisher information:' = info,
  'Standard error:' = std
)
```

## Maximum likelihood for more than one parameter

```{r}
quilpie <- 
  quilpie |> 
  mutate(
    y = factor(y, labels = c('No', 'Yes'))
  )
```

```{r}
p1 <- 
  quilpie |> 
  ggplot(aes(soi,y)) +
  geom_boxplot() +
  labs(x = '', y = '')

p2 <- 
  quilpie |> 
  ggplot(aes(soi, y)) +
  geom_jitter(height = 0.2) +
  labs(x = '', y = '')

p3 <- 
  quilpie |> 
  ggplot(aes(soi, fill = y)) +
  geom_density(position = position_fill(), size = 0) +
  scale_fill_brewer(palette = "Set2") +
  scale_x_continuous(expand = expansion(0)) +
  scale_y_continuous(expand = expansion(0)) +
  labs(y = '') +
  theme(
    legend.position = 'bottom',
    legend.title = element_blank()
  )

(p1 + p2) / p3
```
